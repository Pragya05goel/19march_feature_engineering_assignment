{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11c3ebd8-9ba2-4b78-86cb-0874a5e0c856",
   "metadata": {},
   "source": [
    "# **ASSIGNMENT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903b9a59-58c4-4cc5-ac47-537c05a09ec2",
   "metadata": {},
   "source": [
    "**Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8cea3e-539a-4155-97d0-5176442f4850",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform numeric features into a common range. It rescales the values of a feature to a fixed range, typically between 0 and 1. The purpose of Min-Max scaling is to ensure that all features have the same scale, preventing one feature from dominating or biasing the learning algorithm due to its larger values.\n",
    "\n",
    "The formula to perform Min-Max scaling on a feature x is as follows:\n",
    "\n",
    "x_scaled = (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "Here, x_scaled represents the rescaled value of x, min(x) is the minimum value of x in the dataset, and max(x) is the maximum value of x in the dataset.\n",
    "\n",
    "Let's consider an example to illustrate the application of Min-Max scaling. Suppose we have a dataset with a feature representing the ages of a group of individuals. The ages range from 20 to 60 years. We want to apply Min-Max scaling to this feature.\n",
    "\n",
    "Original ages: [20, 25, 30, 35, 40, 45, 50, 55, 60]\n",
    "\n",
    "To scale these ages using Min-Max scaling, we need to calculate the minimum and maximum values:\n",
    "\n",
    "min(age) = 20\n",
    "max(age) = 60\n",
    "\n",
    "Now, we can apply the formula to obtain the scaled values:\n",
    "\n",
    "Scaled ages:\n",
    "[(20 - 20) / (60 - 20),\n",
    " (25 - 20) / (60 - 20),\n",
    " (30 - 20) / (60 - 20),\n",
    " (35 - 20) / (60 - 20),\n",
    " (40 - 20) / (60 - 20),\n",
    " (45 - 20) / (60 - 20),\n",
    " (50 - 20) / (60 - 20),\n",
    " (55 - 20) / (60 - 20),\n",
    " (60 - 20) / (60 - 20)]\n",
    "\n",
    "Simplified:\n",
    "[0,\n",
    " 0.125,\n",
    " 0.25,\n",
    " 0.375,\n",
    " 0.5,\n",
    " 0.625,\n",
    " 0.75,\n",
    " 0.875,\n",
    " 1]\n",
    "\n",
    "After applying Min-Max scaling, the ages are now transformed into a common range between 0 and 1. This normalization ensures that the age feature is comparable to other features in the dataset, and the values no longer dominate the analysis solely based on their magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b6b32c-d193-4e4d-a8f6-f437024344c2",
   "metadata": {},
   "source": [
    "**Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be085742-7f85-4361-879f-fc48e1c73eee",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as vector normalization or feature scaling, is a data preprocessing technique that rescales the values of a feature to have a unit norm. In other words, it transforms each data point into a vector of length 1 while preserving the direction of the vector. This technique is particularly useful when the magnitude of the data points is not as important as their direction or when dealing with algorithms that are sensitive to the scale of features.\n",
    "\n",
    "To apply the Unit Vector technique, each data point is divided by its Euclidean norm, which is calculated as the square root of the sum of squares of its individual values.\n",
    "\n",
    "The formula to perform Unit Vector scaling on a feature x is as follows:\n",
    "\n",
    "x_scaled = x / ||x||\n",
    "\n",
    "Here, x_scaled represents the rescaled value of x, x represents the original value of the feature, and ||x|| represents the Euclidean norm of x.\n",
    "\n",
    "Let's consider an example to illustrate the application of the Unit Vector technique. Suppose we have a dataset with two features: height (in centimeters) and weight (in kilograms) of individuals. We want to apply Unit Vector scaling to these features.\n",
    "\n",
    "Original data:\n",
    "Height: [165, 170, 175, 180, 185]<br>\n",
    "Weight: [60, 70, 80, 90, 100]<br>\n",
    "\n",
    "To scale these features using the Unit Vector technique, we need to calculate the Euclidean norm for each data point:\n",
    "\n",
    "||[165, 60]|| = sqrt(165^2 + 60^2) = sqrt(27225 + 3600) = sqrt(30825) = 175.68<br>\n",
    "||[170, 70]|| = sqrt(170^2 + 70^2) = sqrt(28900 + 4900) = sqrt(33800) = 183.68<br>\n",
    "||[175, 80]|| = sqrt(175^2 + 80^2) = sqrt(30625 + 6400) = sqrt(37025) = 192.47<br>\n",
    "||[180, 90]|| = sqrt(180^2 + 90^2) = sqrt(32400 + 8100) = sqrt(40500) = 201.25<br>\n",
    "||[185, 100]|| = sqrt(185^2 + 100^2) = sqrt(34225 + 10000) = sqrt(44225) = 210.30<br>\n",
    "\n",
    "Now, we can apply the formula to obtain the scaled values:\n",
    "\n",
    "Scaled height:\n",
    "[165 / 175.68,\n",
    " 170 / 183.68,\n",
    " 175 / 192.47,\n",
    " 180 / 201.25,\n",
    " 185 / 210.30]\n",
    "\n",
    "Scaled weight:\n",
    "[60 / 175.68,\n",
    " 70 / 183.68,\n",
    " 80 / 192.47,\n",
    " 90 / 201.25,\n",
    " 100 / 210.30]\n",
    "\n",
    "Simplified:\n",
    "Height: [0.94, 0.93, 0.91, 0.89, 0.88]<br>\n",
    "Weight: [0.34, 0.38, 0.42, 0.45, 0.48]\n",
    "\n",
    "After applying the Unit Vector technique, both the height and weight features are scaled such that each data point becomes a unit vector with a length of 1. This scaling preserves the direction of the data points while eliminating the effect of their magnitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52e87fb-fcb6-41ba-a7fb-cd10fb2c342f",
   "metadata": {},
   "source": [
    "**Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1b2b3e-aa8d-4d31-9c4c-c6136066d3a0",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a widely used technique in data analysis and machine learning for dimensionality reduction. It aims to transform a high-dimensional dataset into a lower-dimensional space while preserving the most important information or patterns present in the original data.\n",
    "\n",
    "The goal of PCA is to find a new set of orthogonal variables, known as principal components, that capture the maximum variance in the data. These principal components are linear combinations of the original features, and each subsequent component captures as much of the remaining variance as possible. The first principal component accounts for the most significant amount of variation in the data, followed by the second, third, and so on.\n",
    "\n",
    "The steps involved in performing PCA are as follows:\n",
    "\n",
    "1. Standardize the data: It is essential to standardize the data by subtracting the mean and dividing by the standard deviation of each feature. This ensures that all features are on the same scale and prevents dominance by high-variance features.\n",
    "\n",
    "2. Compute the covariance matrix: Calculate the covariance matrix of the standardized data, which represents the relationships between different features. The covariance matrix provides information about the variance and correlation between pairs of features.\n",
    "\n",
    "3. Compute eigenvectors and eigenvalues: Find the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the principal components, while eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "4. Select the principal components: Sort the eigenvalues in descending order and choose the top-k eigenvectors corresponding to the largest eigenvalues. These selected eigenvectors are the principal components that capture the most significant variance in the data.\n",
    "\n",
    "5. Project the data onto the new feature space: Multiply the standardized data by the selected principal components to obtain the transformed dataset in the lower-dimensional space.\n",
    "\n",
    "Let's consider an example to illustrate the application of PCA. Suppose we have a dataset with three features: height, weight, and age of individuals. We want to perform PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "Original data:\n",
    "Height: [165, 170, 175, 180, 185]<br>\n",
    "Weight: [60, 70, 80, 90, 100]<br>\n",
    "Age: [25, 30, 35, 40, 45]<br>\n",
    "\n",
    "1. Standardize the data: Subtract the mean and divide by the standard deviation of each feature.\n",
    "\n",
    "Standardized data:\n",
    "Height: [-1.41, -0.71, 0, 0.71, 1.41]<br>\n",
    "Weight: [-1.41, -0.71, 0, 0.71, 1.41]<br>\n",
    "Age: [-1.41, -0.71, 0, 0.71, 1.41]<br>\n",
    "\n",
    "2. Compute the covariance matrix:\n",
    "\n",
    "Covariance matrix:\n",
    "[[ 1.0, 1.0, 1.0],\n",
    " [ 1.0, 1.0, 1.0],\n",
    " [ 1.0, 1.0, 1.0]]\n",
    "\n",
    "3. Compute eigenvectors and eigenvalues:\n",
    "\n",
    "Eigenvectors: \n",
    "[1/sqrt(3), 1/sqrt(3), 1/sqrt(3)]\n",
    "\n",
    "Eigenvalues: \n",
    "[3.0, 0, 0]\n",
    "\n",
    "4. Select the principal components: Since there is only one non-zero eigenvalue, we choose the corresponding eigenvector as the principal component.\n",
    "\n",
    "Selected principal component:\n",
    "[1/sqrt(3), 1/sqrt(3), 1/sqrt(3)]\n",
    "\n",
    "5. Project the data onto the new feature space: Multiply the standardized data by the principal component.\n",
    "\n",
    "Projected data:\n",
    "[1/sqrt(3), 1/sqrt(3), 1/sqrt(3)]\n",
    "\n",
    "After performing PCA, the dataset is reduced to\n",
    "\n",
    " a single dimension represented by the principal component. The original three-dimensional data is projected onto this one-dimensional space, capturing the maximum variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec53ee-2e60-4bf9-8e0e-b0a8869f46e6",
   "metadata": {},
   "source": [
    "**Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ff743f-fbea-4686-9c6a-71b14d044955",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) can be used for feature extraction, which involves transforming the original features into a new set of features that capture the most important information or patterns in the data. In this context, PCA helps identify the most relevant features or combinations of features that contribute significantly to the variability of the data.\n",
    "\n",
    "The process of using PCA for feature extraction involves the following steps:\n",
    "\n",
    "1. Standardize the data: It is essential to standardize the data by subtracting the mean and dividing by the standard deviation of each feature. This step ensures that all features are on the same scale and prevents dominance by high-variance features.\n",
    "\n",
    "2. Compute the covariance matrix: Calculate the covariance matrix of the standardized data, which represents the relationships between different features. The covariance matrix provides information about the variance and correlation between pairs of features.\n",
    "\n",
    "3. Compute eigenvectors and eigenvalues: Find the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the principal components, while eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "4. Select the principal components: Sort the eigenvalues in descending order and choose the top-k eigenvectors corresponding to the largest eigenvalues. These selected eigenvectors are the principal components that capture the most significant variance in the data.\n",
    "\n",
    "5. Project the data onto the new feature space: Multiply the standardized data by the selected principal components to obtain the transformed dataset in the lower-dimensional space. These transformed features, known as the principal component scores, can be used as the extracted features representing the original data.\n",
    "\n",
    "The relationship between PCA and feature extraction lies in the fact that PCA extracts new features (principal components) that are linear combinations of the original features. These principal components are chosen to capture the maximum variance in the data. By selecting a subset of the principal components, we can effectively reduce the dimensionality of the dataset while retaining the most important information.\n",
    "\n",
    "Let's consider an example to illustrate the concept of using PCA for feature extraction. Suppose we have a dataset with five features: A, B, C, D, and E. We want to use PCA to extract the most relevant features from this dataset.\n",
    "\n",
    "Original data:\n",
    "A: [1, 2, 3, 4, 5]<br>\n",
    "B: [5, 4, 3, 2, 1]<br>\n",
    "C: [1, 1, 1, 1, 1]<br>\n",
    "D: [0, 0, 0, 0, 0]<br>\n",
    "E: [2, 2, 2, 2, 2]<br>\n",
    "\n",
    "1. Standardize the data: Subtract the mean and divide by the standard deviation of each feature.\n",
    "\n",
    "2. Compute the covariance matrix:\n",
    "\n",
    "3. Compute eigenvectors and eigenvalues:\n",
    "\n",
    "4. Select the principal components:\n",
    "\n",
    "5. Project the data onto the new feature space:\n",
    "\n",
    "The transformed dataset obtained after projecting the original data onto the principal components will serve as the extracted features, representing the most important information in the data. These extracted features can be used for subsequent analysis or machine learning tasks, potentially reducing the dimensionality of the dataset and improving computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4a3f4c-dc33-4d17-b9a5-97341174b23a",
   "metadata": {},
   "source": [
    "**Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f64a6-0dd4-4a5e-86f0-30588af546ed",
   "metadata": {},
   "source": [
    "To preprocess the data for building a recommendation system for a food delivery service, we can use Min-Max scaling on certain features such as price, rating, and delivery time. Here's how Min-Max scaling can be applied:\n",
    "\n",
    "1. Identify the features: In this case, the features of interest are price, rating, and delivery time.\n",
    "\n",
    "2. Determine the range: Decide on the desired range for the scaled values. It is common to scale features between 0 and 1 using Min-Max scaling.\n",
    "\n",
    "3. Calculate the minimum and maximum values: Find the minimum and maximum values for each feature in the dataset. For example, for the price feature, find the minimum and maximum prices across all food items or restaurants in the dataset.\n",
    "\n",
    "4. Apply Min-Max scaling formula: Use the Min-Max scaling formula to scale the values of each feature:\n",
    "\n",
    "   x_scaled = (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "   Here, x represents the original value of the feature, min(x) is the minimum value of the feature, and max(x) is the maximum value of the feature.\n",
    "\n",
    "5. Perform Min-Max scaling: Apply the Min-Max scaling formula to each value of the feature in the dataset, ensuring that the scaling is performed independently for each feature.\n",
    "\n",
    "The purpose of using Min-Max scaling in this context is to bring all the features within the same range (0 to 1) so that they contribute equally to the recommendation algorithm. By scaling the features, we prevent any single feature (e.g., price) from dominating the recommendation process solely based on its larger values. This normalization ensures that all features are on a comparable scale and have equal importance in the recommendation system.\n",
    "\n",
    "For example, let's say we have a dataset of food items with the following features:\n",
    "\n",
    "Price: [10, 20, 15, 25, 30]\n",
    "Rating: [3.5, 4.2, 3.9, 4.8, 4.0]\n",
    "Delivery Time: [30, 25, 20, 35, 40]\n",
    "\n",
    "To use Min-Max scaling, we calculate the minimum and maximum values for each feature:\n",
    "\n",
    "Price: min = 10, max = 30\n",
    "Rating: min = 3.5, max = 4.8\n",
    "Delivery Time: min = 20, max = 40\n",
    "\n",
    "Then, we apply the Min-Max scaling formula to each feature:\n",
    "\n",
    "Scaled Price: [(10 - 10) / (30 - 10), (20 - 10) / (30 - 10), (15 - 10) / (30 - 10), (25 - 10) / (30 - 10), (30 - 10) / (30 - 10)]\n",
    "Scaled Rating: [(3.5 - 3.5) / (4.8 - 3.5), (4.2 - 3.5) / (4.8 - 3.5), (3.9 - 3.5) / (4.8 - 3.5), (4.8 - 3.5) / (4.8 - 3.5), (4.0 - 3.5) / (4.8 - 3.5)]\n",
    "Scaled Delivery Time: [(30 - 20) / (40 - 20), (25 - 20) / (40 - 20), (20 - 20) / (40 - 20), (35 - 20) / (40 - 20), (40 - 20) / (40 - 20)]\n",
    "\n",
    "The resulting scaled values will be in the range of 0 to 1, representing the normalized values of\n",
    "\n",
    " each feature. These scaled features can then be used as inputs for building the recommendation system, ensuring that they are on a consistent and comparable scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb13cca-2f33-4ec7-bdf6-9ee033de21f8",
   "metadata": {},
   "source": [
    "**Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3406ca5f-05ec-4d67-9953-998f577be6a5",
   "metadata": {},
   "source": [
    "When working on a project to predict stock prices with a dataset containing numerous features, PCA (Principal Component Analysis) can be utilized to reduce the dimensionality of the dataset. Reducing the dimensionality can be beneficial in stock price prediction as it helps to mitigate the curse of dimensionality, enhance computational efficiency, and identify the most informative features.\n",
    "\n",
    "Here's an overview of how PCA can be used to reduce the dimensionality of the dataset in the context of stock price prediction:\n",
    "\n",
    "1. Identify the features: Identify the features in the dataset that include company financial data and market trends. For example, features could include stock volume, earnings per share, price-to-earnings ratio, interest rates, market indices, etc.\n",
    "\n",
    "2. Standardize the data: It is essential to standardize the data by subtracting the mean and dividing by the standard deviation of each feature. This step ensures that all features are on the same scale and prevents dominance by high-variance features.\n",
    "\n",
    "3. Compute the covariance matrix: Calculate the covariance matrix of the standardized data. The covariance matrix represents the relationships and dependencies between different features, providing insights into their pairwise variations.\n",
    "\n",
    "4. Compute eigenvectors and eigenvalues: Find the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the principal components, while eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "5. Select the principal components: Sort the eigenvalues in descending order and choose the top-k eigenvectors corresponding to the largest eigenvalues. These selected eigenvectors are the principal components that capture the most significant variance in the data.\n",
    "\n",
    "6. Project the data onto the new feature space: Multiply the standardized data by the selected principal components to obtain the transformed dataset in the lower-dimensional space. These transformed features, known as the principal component scores, can be used as the reduced-dimensional representation of the original data.\n",
    "\n",
    "By performing PCA, the high-dimensional dataset is transformed into a lower-dimensional space, where each principal component represents a linear combination of the original features. The selected principal components are those that capture the most variance in the dataset, providing a compressed representation of the data while retaining the most important information.\n",
    "\n",
    "The reduced-dimensional dataset obtained through PCA can then be used as input for building a stock price prediction model. The dimensionality reduction helps to focus on the most influential features, reduce noise, and improve model efficiency and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d9fde9-def4-446d-8411-a304e99eab14",
   "metadata": {},
   "source": [
    "**Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377a1594-01be-4210-a4e6-05ec6fabed58",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling on the dataset [1, 5, 10, 15, 20] and transform the values to a range of -1 to 1, we can follow these steps:\n",
    "\n",
    "1. **Find the minimum and maximum values**: Determine the minimum and maximum values in the dataset. In this case, the minimum value is 1, and the maximum value is 20.\n",
    "\n",
    "2. **Apply the Min-Max scaling formula**: Apply the Min-Max scaling formula to each value in the dataset using the minimum and maximum values:\n",
    "\n",
    "   scaled_value = (original_value - minimum_value) / (maximum_value - minimum_value)\n",
    "\n",
    "3. **Calculate scaled values**: Calculate the scaled values for each element in the dataset using the Min-Max scaling formula:\n",
    "\n",
    "   For 1:  scaled_value = (1 - 1) / (20 - 1) = 0 / 19 = 0<br>\n",
    "   For 5:  scaled_value = (5 - 1) / (20 - 1) = 4 / 19 ≈ 0.2105<br>\n",
    "   For 10: scaled_value = (10 - 1) / (20 - 1) = 9 / 19 ≈ 0.4737<br>\n",
    "   For 15: scaled_value = (15 - 1) / (20 - 1) = 14 / 19 ≈ 0.7368<br>\n",
    "   For 20: scaled_value = (20 - 1) / (20 - 1) = 19 / 19 = 1<br>\n",
    "\n",
    "4. **Transform scaled values to the range of -1 to 1**: The scaled values obtained in the previous step range from 0 to 1. To transform them to the range of -1 to 1, apply the following formula:\n",
    "\n",
    "   scaled_value_in_range = (scaled_value * 2) - 1\n",
    "\n",
    "5. **Calculate transformed values**: Calculate the transformed values for each scaled value using the formula mentioned above:\n",
    "\n",
    "   For 0:        transformed_value = (0 * 2) - 1 = -1<br>\n",
    "   For 0.2105:   transformed_value = (0.2105 * 2) - 1 ≈ -0.5789<br>\n",
    "   For 0.4737:   transformed_value = (0.4737 * 2) - 1 ≈ -0.0526<br>\n",
    "   For 0.7368:   transformed_value = (0.7368 * 2) - 1 ≈ 0.4737<br>\n",
    "   For 1:        transformed_value = (1 * 2) - 1 = 1\n",
    "\n",
    "The transformed values for the dataset [1, 5, 10, 15, 20] after Min-Max scaling to a range of -1 to 1 are approximately [-1, -0.5789, -0.0526, 0.4737, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc3fec07-12e1-4a6d-b40e-c650f1942225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9999999999999999\n",
      "-0.5789473684210525\n",
      "-0.05263157894736836\n",
      "0.47368421052631593\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Reshape the dataset to a 2D array as required by scikit-learn\n",
    "data = [[1], [5], [10], [15], [20]]\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "for scaled_value in scaled_data:\n",
    "    print(scaled_value[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaa76de-cd75-404d-ad67-5bad87d87a90",
   "metadata": {},
   "source": [
    "**Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c1f30e-f947-4581-b532-37ed3a493261",
   "metadata": {},
   "source": [
    "To perform feature extraction using PCA on a dataset with features [height, weight, age, gender, blood pressure], we need to follow these steps:\n",
    "\n",
    "1. Standardize the data: Subtract the mean and divide by the standard deviation of each feature to ensure all features are on the same scale.\n",
    "\n",
    "2. Compute the covariance matrix: Calculate the covariance matrix of the standardized data to understand the relationships between different features.\n",
    "\n",
    "3. Compute eigenvectors and eigenvalues: Find the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the principal components, and eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "4. Select the principal components: Sort the eigenvalues in descending order and choose the top-k eigenvectors corresponding to the largest eigenvalues. These eigenvectors are the principal components that capture the most significant variance in the data.\n",
    "\n",
    "The number of principal components to retain depends on the desired level of variance explained and the trade-off between dimensionality reduction and information loss. In practice, a common approach is to consider the cumulative explained variance.\n",
    "\n",
    "To determine the number of principal components to retain, we can analyze the explained variance ratio, which represents the proportion of the total variance explained by each principal component.\n",
    "\n",
    "Let's assume that after performing PCA on the given dataset, we obtain the following eigenvalues and their explained variance ratio:\n",
    "\n",
    "Eigenvalues: [3.5, 2.8, 1.9, 1.2, 0.6]\n",
    "Explained Variance Ratio: [0.35, 0.28, 0.19, 0.12, 0.06]\n",
    "\n",
    "To decide the number of principal components to retain, we can calculate the cumulative explained variance ratio by summing the explained variance ratios from the first principal component onwards:\n",
    "\n",
    "Cumulative Explained Variance Ratio: [0.35, 0.63, 0.82, 0.94, 1.00]\n",
    "\n",
    "In this case, the cumulative explained variance ratio reaches 1.00 after considering all five principal components. It indicates that all the variance in the dataset can be explained by these five components.\n",
    "\n",
    "To determine how many principal components to retain, we typically look for a threshold value of explained variance ratio. For example, if we set a threshold of 0.95, we would choose the minimum number of principal components that collectively explain at least 95% of the variance. In this case, we can observe that the first three principal components alone explain 82% of the variance. Hence, we might choose to retain the first three principal components to strike a balance between dimensionality reduction and retaining significant variance.\n",
    "\n",
    "However, the final decision on the number of principal components to retain also depends on the specific requirements, domain knowledge, and the performance of the subsequent analysis or model using the reduced feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b30a7cb-51a9-48f5-a1fa-2726a3735117",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8847bd66-95ef-4baa-ba62-f51137405cde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
